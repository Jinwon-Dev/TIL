> **최신 컴퓨터의 기본 대용량 저장장치 시스템은 보조저장장치이며, 일반적으로 HDD 및 NVM 장치를 사용하여 제공된다.**

- **HDD, NVM 장치** 이 두 가지 유형의 저장장치에 관해 알아본다.
- 성능을 최대화하기 위해 **I/O 순서를 스케줄 하는 스케줄링 알고리즘**을 고려한다.
- **장치 포맷팅과 부트 블록, 손상된 블록 및 스왑 공간의 관리**에 대해 알아본다.
- **RAID 시스템**의 구조를 살펴본다.

---

## 1. 대용량 저장장치 구조의 개관

→ ***최신 컴퓨터를 위한 대량의 보조저장장치는 하드 디스크 드라이브(HDD) 및 비휘발성 메모리(NVM) 장치에 의해 관리된다.***

- 이번 절에서는 이러한 장치의 기본 기법, 운영체제가 주소 매핑을 통해 물리적 속성을 논리 저장장치로 변환하는 방법을 설명한다.

</br>

### 하드 디스크 드라이브

→ ***개념적으로 HDD는 비교적 단순하다.***

<img width="434" alt="image" src="https://user-images.githubusercontent.com/106216912/222909010-034984df-2c7f-4fa1-a808-6e631c4baecc.png">

- 각 디스크의 **플래터(platter)** 는 CD처럼 생긴 원형 평판 모양이다.
    - 플래터의 양쪽 표면은 자기 테이프와 유사하게 자기 물질로 덮여 있다.
    - 우리는 정보를 플래터상에 자기적으로 기록하여 저장하고, 플래터의 자기 패턴을 감지하여 정보를 읽는다.

</br>

> **읽기-쓰기 헤드는 모든 플래터의 각 표면 바로 위에서 움직인다.**

- 헤드는 모든 헤드를 한꺼번에 이동시키는 **디스크 암(disk arm)** 에 부착되어 있다.
    - 플래터의 표면은 원형인 **트랙(track)** 으로 논리적으로 나누어져 있고, 이것은 다시 **섹터(sector)** 로 나누어진다.
    - 트랙의 집합은 하나의 **실린더(cylinder)** 를 형성한다.

</br>

> **디스크 드라이브 모터는 고속으로 회전한다.**

- 대부분의 드라이브는 **분당 회전수(RPM)** 단위로 표현되며, 초당 60~250회 회전한다.
    - 일반 드라이브는 5,400, 7,200, 10,000 및 15,000 RPM에서 회전한다.

- 회전 속도는 전송 속도와 관련이 있다.
    - **전송 속도** : 드라이브와 컴퓨터 간의 데이터 흐름의 속도
    - **액세스 시간**
        - **탐색 시간** : 디스크 암을 원하는 실린더로 이동하는 데 필요한 시간
        - **회전 지연시간** : 원하는 섹터가 디스크 헤드 위치까지 회전하는 데 걸리는 시간

</br>

> **디스크 헤드는 공기 또는 헬륨과 같은 다른 가스의 매우 얇은 쿠션 위를 비행하며 헤드가 디스크 표면에 닿을 위험이 있다.**

- **헤드 충돌** : 디스크 플래터는 얇은 보호 층으로 코팅되어 있지만, 헤드는 때때로 자기 표면을 손상한다.
    - 일반적으로 수리할 수 없고, 전체 디스크를 교체해야 한다.

</br>

### 비휘발성 메모리 장치

→ ***비휘발성 메모리(NVM) 장치의 중요성이 증가하고 있다.***

</br>

> **비휘발성 메모리 장치 개요**

→ ***플래시 메모리 기반 NVM은 디스크 드라이브와 유사한 컨테이너에서 자주 사용되며, SSD라고 한다.***

- 다른 경우에는 USB 드라이브 또는 DRAM 스틱의 형태를 취한다.

<img width="345" alt="image" src="https://user-images.githubusercontent.com/106216912/222909388-23aaffbf-0c40-4bff-969a-208057d32426.png">

</br>

> **NVM 장치는 움직이는 부품이 없으므로 HDD보다 안정성이 높으며, 탐색 시간이나 회전 지연시간이 없으므로 더 빠를 수 있다!**
> 
- 또한, **전력 소비량**도 적다.
    - **단점 : 기존 하드 디스크보다 비싸고, 용량이 적다.**

</br>

→ 하지만, **NVM 장치의 용량이 HDD 용량보다 빠르게 증가하고, 가격은 빠르게 하락하여 사용량이 급격히 증가하고 있다.**

- 일부 랩탑에서는 더 작고, 빠르고, 에너지 효율적이다.

</br>

> **NVM 장치는 하드 디스크 드라이브보다 훨씬 빠를 수 있으므로, 표준 버스 인터페이스가 처리량을 크게 제한할 수 있다.**
> 
- 일부 NVM 장치는 시스템 버스에 직접 연결되도록 설계되었다.
    - 일부 시스템은 디스크 드라이브를 직접 대체하는 데 사용하는 반면, 다른 시스템은 새로운 캐시 계층을 사용하여 성능을 최적화하기 위해 자기 디스크, NVM 및 메인 메모리 간에 데이터를 이동한다.

</br>

> **NAND 반도체는 일부 특성 때문에 자체적인 저장 및 신뢰성 문제를 가진다.**
> 
- 섹터와 유사한 “페이지” 단위로 읽고 쓸 수 있지만, 데이터를 덮어쓸 수는 없다.
    - 덮어쓰기 위해서는 NAND 셀을 먼저 지워야 한다.

</br>

- 쓰기 마모로 인해 그리고 움직이는 부품이 없기 때문에 NAND NVM 수명은 연 단위가 아니라 **DWPD(Drive Writes Per Day)** 로 측정된다.
    - **드라이브 장애가 발생하기 전에 드라이브 용량에 해당하는 데이터를 하루에 몇 번 쓸 수 있는지**를 나타낸다.

</br>

→ 이러한 제한으로 인해, 여러 가지 개선 알고리즘이 고안되었다!

- 일반적으로 NVM 장치 컨트롤러에서 구현되며, 운영체제가 신경 쓰지 않아도 된다.
    - 운영체제는 단순히 논리 블록을 읽고 쓰고, 장치가 그 연산을 관리한다.

</br>

> **NAND 플래시 컨트롤러 알고리즘**
> 
→ ***NAND 반도체는 한 번 쓴 후에는 덮어쓸 수 없으므로, 일반적으로 유효하지 않은 데이터가 포함된 페이지가 있다.***

- 한 번 기록된 후, 나중에 다시 기록된 파일 시스템 블록을 고려하자.
    - 그 사이에 삭제가 발생하지 않은 경우 처음 기록된 페이지에는 이전 데이터가 저장되어 있으며, 현재는 유효하지 않은 데이터이다.

</br>

<img width="346" alt="image" src="https://user-images.githubusercontent.com/106216912/222909788-a52bbd08-c7e2-4d3a-a37b-9411432ba637.png">

- 유효한 데이터와 유효하지 않은 페이지를 포함하는 **NAND 블록**이다.
    - 유효한 데이터를 포함하는 논리 블록을 추적하기 위해 컨트롤러는 **플래시 변환 계층(FTL)** 을 유지한다.
        - **이 테이블은 현재 유효한 논리 블록을 포함하는 물리 페이지를 매핑한다.**

</br>

> **보류 중인 쓰기 요청이 있는 전체 SSD를 고려하자.**
> 
- SSD가 가득 찼기 때문에 모든 페이지가 기록되었지만, 그 중 유효하지 않은 데이터만 포함하고 있는 블록이 있을 수 있다.
    - 개별 페이지에 유효하지 않은 데이터가 있으면 여전히 사용 가능한 공간이 있을 수 있다.
    - 이 경우, **가비지 수집**이 발생할 수 있다.

</br>

- 그러나 가비지 수집은 유효한 데이터를 어디에 저장해야 할까?
    - 이 문제를 해결하고 쓰기 성능을 향상하기 위해 NVM 장치는 **과잉 공급**을 사용한다.
    - **장치는 언제나 쓸 수 있도록 다수의 페이지를 따로 준비해 놓는다.**

</br>

> **과잉 공급 공간은 마모 평준화(wear leveling)에도 도움이 된다.**
> 
- 다른 블록에 비해 일부 블록만 반복적으로 지워지면 자주 지워지는 블록은 다른 블록보다 빨리 마모된다.
    - 그리고, 모든 블록이 동시에 마모되는 경우보다 전체 장치의 수명이 짧아진다.

</br>

> **데이터 보호 측면에서 NVM 장치는 오류 수정 코드를 제공한다.**
> 
- 이 코드는 기록될 때 계산되어 데이터와 함께 저장되고, 읽을 때 데이터와 함께 읽어 오류를 감지하고 수정한다.

</br>

### 휘발성 메모리

→ ***RAM 드라이브는 보조저장장치처럼 작동하지만, 시스템 DRAM의 한 영역을 할당하여 저장장치인 것처럼 나머지 시스템에 제공하는 장치 드라이버에 의해 생성된다.***

- DRAM은 휘발성이며, RAM 드라이브의 데이터는 시스템 크래시, 종료 또는 전원이 꺼진 후에는 지속되지 않는다.
    - RAM 드라이브를 사용하면 사용자와 프로그래머가 **표준 파일 연산을 사용하여 데이터를 메모리에 임시로 보관할 수 있다.**

</br>

> **RAM 드라이브는 고속 임시 저장 공간으로 유용하다.**
> 
- DRAM은 NVM 장치보다 **훨씬 빠르다.**
    - 또한, RAM 드라이브에 대한 I/O 작업은 파일과 내용을 생성, 읽기, 쓰기 및 삭제하는 가장 빠른 방법이다.
    - 많은 프로그램이 임시 파일을 저장하기 위해 RAM 드라이브를 사용하거나 혜택을 얻을 수 있다.

</br>

### 보조저장장치 연결 방법

→ ***보조저장장치는 시스템 버스 또는 I/O 버스에 의해 컴퓨터에 연결된다.***

- **ATA, SATA, eSATA, SAS, USB, FC**를 포함하여 여러 종류의 버스를 사용할 수 있다.
    - NVM 장치는 HDD보다 훨씬 빠르기 때문에, 산업계에서는 **NVMe**라는 NVM 장치를 위한 특별하고 빠른 인터페이스를 만들었다.
        - NVMe는 **장치를 시스템 PCI 버스에 직접 연결하여 다른 연결 방법과 비교해 처리량을 높이고, 지연시간을 줄인다.**

</br>

> **버스에서의 데이터 전송은 컨트롤러라고 하는 특수 전자 프로세서에 의해 수행된다.**
> 
- **호스트 컨트롤러**는 버스의 컴퓨터 쪽에 있는 컨트롤러이다.
    - 각 저장장치에는 **장치 컨트롤러**가 내장되어 있다.

</br>

### 주소 매핑

→ ***저장장치는 논리 블록의 커다란 1차원 배열처럼 주소가 매겨진다.***

- 논리 블록은 가장 작은 전송 단위이고, 각 논리 블록은 물리 섹터 또는 반도체 페이지로 매핑된다.
    - 논리 블록의 1차원 배열은 장치의 섹터들 또는 페이지들에 매핑된다.

</br>

- 매핑은 해당 트랙을 순서대로 완료한 후에, 해당 실린더의 나머지 트랙을 매핑한 다음 나머지 실린더를 바깥쪽에서 안쪽 순으로 매핑한다.
    - NVM의 경우 칩, 블록 및 페이지의 튜플에서 논리 블록의 배열로 매핑된다.
    - **논리 블록 주소(LBA)** 를 사용하기가 더 쉽다.

</br>

> **현실적으로는 세 가지 이유로 이 변환을 수행하는 것이 어렵다!**
> 
1. 대부분의 드라이브에는 결함이 있는 섹터가 있지만, 매핑은 드라이브의 다른 곳에 있는 예비 섹터로 대체하여 이를 숨긴다.
2. 트랙당 섹터 수는 일부 드라이브에서 일정하지 않다.
3. 디스크 제조업체는 LBA와 물리 주소 간의 매핑을 내부적으로 관리하므로, 현재 드라이브에서는 LBA와 물리적 섹터 간에 관계가 거의 없다.

</br>

> **두 번째 이유를 좀 더 살펴보면, 고정 선형 속도(CLV)를 사용하는 장치에서는 트랙당 비트의 밀도가 일정하다.**
> 
- 트랙이 디스크의 중심으로부터 멀어질수록 트랙은 더 길이가 길어져 더 많은 섹터를 가질 수 있게 된다.
    - 따라서, 현대의 디스크는 실린더들을 몇 개의 구역으로 나눈다.
    - 드라이브는 헤드가 바깥쪽에서 안쪽 트랙으로 이동하면서 헤드 아래를 통과하는 데이터의 비율을 동일하게 유지하기 위해 회전 속도를 늘린다.

</br>

→ 이의 대안으로, **디스크의 회전 속도를 일정하게 유지하고, 이 경우 안쪽 트랙에서 바깥쪽 트랙으로 갈수록 비트의 밀도를 줄여 데이터 비율을 일정하게 유지할 수 있다.**

- 이 방법은 하드 디스크에 사용되며, **고정 각 속도(CAV)** 로 알려져 있다.

---

## 2. 디스크 스케줄링

→ ***운영체제의 책임 중 하나는 효율적인 하드웨어 사용이다.***

- HDD 또는 플래터를 사용하는 기계식 저장장치의  경우, 접근 시간은 두 가지 요소로 이루어진다.
    - **탐색 시간** : 장치 암이 헤드를 해당 실린더로 움직이는 데 걸리는 시간
    - **회원 지연 시간** : 플래터가 원하는 섹터를 헤드 위치까지 회전시키는 데 소요되는 추가적인 시간
    - **장치 대역폭** : 전송된 총 바이트 수를 첫 번째 서비스 요청과 마지막 전송 완료 사이의 전체 시간으로 나눈 값

</br>

> **프로세스가 입출력을 해야 할 때마다 운영체제에 시스템 콜을 한다.**
> 
- 이 호출에는 여러 가지의 인수가 주어진다.
    - 이 작업이 입력 또는 출력인지의 여부
    - 연산이 수행될 파일을 가리키는 열린 파일 핸들
    - 전송을 위한 메모리 주소
    - 전송할 데이터의 양

</br>

> **원하는 드라이브와 컨트롤러가 쉬고 있다면, 이 요청은 즉시 시작된다.**
> 
- 그러나 드라이브나 컨트롤러가 바쁘면, 이 요청은 드라이브의 큐에 들어가 기다려야 한다.
    - 이 큐에는 여러 장치 I/O 요청들이 함께 대기하고 있을 수 있다.

</br>

→ **헤드 탐색을 피하면 성능을 최적화할 수 있는 장치에 대해 요청 큐를 유지하면, 장치 드라이버는 큐의 순서를 조정하여 성능을 향상할 수 있는 기회를 가지게 된다.**

</br>

### 선입 선처리 스케줄링

→ ***디스크 스케줄링의 가잔 간단한 형태는 선입 선처리(FCFS)이다.***

- 공평해 보이지만, 빠른 서비스를 제공하지는 못한다.

<img width="430" alt="image" src="https://user-images.githubusercontent.com/106216912/222914028-d3413c3e-1ca3-4201-8644-be48092c018d.png">

</br>

### SCAN 스케줄링

→ ***SCAN 알고리즘에서는 디스크 암이 디스크의 한쪽에서 시작하여 다른 끝으로 이동하며, 가는 길에 있는 요청을 모두 처리한다.***

- **다른 한쪽 끝에 도달하면 역방향으로 이동하면서 오는 길에 있는 요청을 모두 처리한다.**
    - 따라서, 헤드는 디스크 양쪽을 계속해서 가로지르며 왕복한다.
    - **엘리베이터 알고리즘**이라고도 부른다.

<img width="434" alt="image" src="https://user-images.githubusercontent.com/106216912/222914051-7451302e-e431-4f31-85ba-1429d35f18fd.png">

</br>

### C-SCAN 스케줄링

→ ***C-SCAN 스케줄링은 각 요청에 걸리는 시간을 좀 더 균등하게 하기 위한 SCAN의 변형이다.***

- 한 쪽으로 헤드를 이동해 가면서 요청을 처리하지만, 한쪽 끝에 다다르면 처음에 시작했던 자리로 돌어가서 서비스를 시작한다.

<img width="439" alt="image" src="https://user-images.githubusercontent.com/106216912/222914085-e94526a4-d678-4745-9edc-6ad041576220.png">

</br>

### 디스크 스케줄링 알고리즘의 선택

→ ***SCAN 및 C-SCAN은 기아 문제를 일으킬 가능성이 작기 때문에 디스크에 많은 부하를 주는 시스템의 성능을 향상한다.***

- 그래도 기아가 계속될 수 있으며, 이로 인해 Linux가 **마감시간 스케줄러**를 만들었다.
    - 이 스케줄러는 읽기와 쓰기별로 큐를 유지 관리하며, 읽기 연산에 높은 우선순위를 준다.
    - 대기열은 LBA 순서로 정렬되어 기본적으로 C-SCAN을 구현하고, 모든 I/O 요청은 이 LBA 순서로 묶어서 전송된다.

</br>

> **RHEL 7에는 다른 두 가지가 포함되어 있다.**
> 
- **NOOP**은 NVM 장치와 같은 빠른 저장장치를 사용하는 CPU 중심 시스템에 선호된다.
    - **CFQ 스케줄러**는 SATA 드라이브의 디폴트 스케줄러이다.
    - CFQ는 실시간, 최선 노력 및 유휴의 세 가지 큐를 유지한다.

---

## 3. NVM 스케줄링

→ ***NVM 장치에는 이동 디스크 헤드가 없으며, 일반적으로 간단한 FCFS 정책을 사용한다.***

- ex) Linux NOOP 스케줄러는 FCFS 정책을 사용하지만, 인접한 요청을 병합하도록 수정한다.

</br>

> **I/O는 순차적으로 또는 무작위로 발생할 수 있다.**
> 
- 읽거나 쓸 데이터가 읽기/쓰기 헤드 근처에 있기 때문에 HDD 및 테이프와 같은 기계 장치에는 순차적 액세스가 최적이다.
    - 초당 입/출력 연산 수(IOPS)로 측정되는 무작위 액세스 I/O는 HDD 디스크 헤드 이동을 유발한다.

</br>

> **HDD 헤드 탐색을 최소화하고 미디어에 대한 데이터 읽기 및 쓰기가 강조되는 `raw` 순차 처리량 측면에서는, NVM이 더 적은 이득을 제공한다.**
> 
- 이러면 읽기의 경우, 두 가지 유형의 장치 성능은 동등하거나 NVM이 10배 정도 이득을 얻는다.
    - NVM에 쓰는 것이 읽는 것보다 느리므로 이러한 이득은 줄어든다.

</br>

→ 시간이 지남에 따라 NVM 장치의 수명과 성능을 향상하는 한 가지 방법?

- **파일이 삭제될 때 파일 시스템이 장치에 알리도록 하여, 장치가 해당 파일이 저장된 블록을 지울 수 있도록 하는 것**

</br>

> **가비지 수집이 성능에 미치는 영향**
> 
- **하나의 쓰기 요청으로 인해 결국 페이지 쓰기, 하나 이상의 페이지 읽기 및 하나 이상의 페이지 쓰기가 발생한다.**
    - 응용 프로그램이 아니라 가비지 수집 및 공간 관리를 수행하는 NVM 장치에 의한 I/O 요청 생성을 **쓰기 증폭**이라고 한다.
        - 장치의 쓰기 성능이 큰 영향을 준다!

---

## 4. 오류 감지 및 수정

→ ***오류 감지 및 수정은 메모리, 네트워킹 및 저장장치를 포함한 많은 컴퓨팅 영역에서 필수적이다.***

- **오류 감지**는 문제가 발생했는지 여부를 결정한다.

</br>

> **메모리 시스템은 패리티 비트를 사용하여 특정 오류를 오랫동안 감지했다.**
> 
- 메모리 시스템의 각 바이트에는 1로 설정된 비트 수가 짝수인지 홀수인지 기록하는 패리티 비트가 연관된다.
    - 바이트의 비트 중 하나가 손상되면, 바이트의 패리티가 변경되어 저장된 패리티와 일치하지 않는다.
        - 따라서, 모든 단일 비트 오류는 메모리 시스템에 의해 감지된다!

</br>

> **패리티는 고정 길이 워드의 값을 계산, 저장 및 비교하기 위하여 나머지 연산을 수행하는 체크섬의 한 형태이다.**
> 
- 네트워킹에서 일반적으로 사용되는 또 다른 오류 감지 방법은 해시 함수를 사용해서 다중 비트 오류를 감지하는 **순환 중복 검사(CRC)** 이다.

</br>

> **오류 수정 코드(ECC)는 문제를 감지할 뿐만 아니라, 보정한다.**
> 
- 보정은 알고리즘과 저장장치를 추가로 사용해서 수행된다.
    - 코드는 필요한 추가 저장장치의 양과 보정할 수 있는 오류 개수에 따라 다르다.

---

## 5. 저장장치 관리

→ ***운영체제는 이 외에도 저장장치 관리와 관련하여 몇 가지 책임을 져야 한다.***

</br>

### 드라이브 포매팅, 파티션, 볼륨

→ ***새로운 저장장치는 아무런 정보가 없는 비어있는 판, 또는 초기화되지 않은 반도체 저장셀의 집합이다.***

- 저장장치는 자료를 저장하기 전에 컨트롤러가 읽고 쓸 수 있도록 섹터들로 나누어져 있어야 한다.
    - NVM 페이지는 초기화되어야 하고, FTL이 생성되어야 한다.
    - 이 과정을 **저수준 포매팅** 또는 **물리적 포매팅**이라고 부른다.

</br>

- 저수준 포매팅은 각 저장장치 위치마다 특별한 자료구조로 장치를 채운다.
    - 섹터 또는 페이지를 위한 자료구조는 보통 헤더, 자료 구역과 트레일러로 구성된다.

</br>

> **대부분의 하드 디스크는 공장에서 이미 저수준 포매팅이 되어 나온다.**
> 
- 이러한 저수준 포매팅이 되어 있어야, 하드 디스크를 제조하면서 디스크 품질을 검사할 수도 있고, 논리적 블록 주소를 손상되지 않은 섹터 또는 페이지들에게 할당해 줄 수도 있다.
    - 여러 섹터 크기 중에서 하나를 선택하는 것이 가능하다.

</br>

> **드라이브를 사용해서 파일을 보유하려면 운영체제가 여전히 자체 데이터 구조를 장치에 기록해야 한다.**
> 
→ ***운영체제는 이 작업을 세 단계로 수행한다.***

1. 장치를 하나 이상의 블록 또는 페이지 그룹으로 **파티션**한다.
2. **볼륨** 생성 및 관리
3. **논리적 포매팅** 또는 파일 시스템의 생성

</br>

> **효율성을 높이기 위해 대부분의 파일 시스템은 블록을 종종 클러스터라고 하는 더 큰 청크로 묶는다.**
> 
- 파일 시스템 I/O는 **클러스터**를 통해 수행되므로, 효과적으로 I/O가 순차 접근을 더 많이 하고 랜덤 액세스 특성을 줄이는 것을 보장한다.

</br>

> **raw 디스크**
> 
- 일부 운영체제는 특정 프로그램이 파일 시스템 자료구조 없이도 파티션을 논리 블록의 대용량 순차 배열처럼 사용할 수 있게 한다.

</br>

### 부트 블록

→ ***컴퓨터의 전원을 켜거나 재부팅 할 때와 같이 컴퓨터가 실행을 시작하려면, 실행할 초기 프로그램이 존재해야 한다.***

- 이 초기 부트스트랩 로더는 단순한 경향이 있다.
    - 대부분의 컴퓨터에서 부트스트랩은 시스템의 마더보드의 NVM 플래시 메모리 펌웨어에 저장되며, 알려진 메모리 위치에 매핑된다.
    - 부트 파티션이 있는 장치를 **부트 디스크**, 또는 **시스템 디스크**라고 한다.

</br>

> **부트스트랩 NVM의 코드는 저장장치 컨트롤러에 부트스트랩 프로그램을 메모리에 올리도록 지시하고, 그 프로그램의 수행을 시작한다.**

<img width="333" alt="image" src="https://user-images.githubusercontent.com/106216912/222916459-fe8ec6a7-41a5-4ae8-b2fe-632bebc2bc6a.png">

- 부트스트랩 프로그램 본체는 부트스트랩 로더보다 복잡하며, 그 임무도 저장장치 내 임의의 장소에 저장된 운영체제 커널을 찾아내고 그것을 시작시키는 일까지 하게 된다.

</br>

### 손상된 블록

→ ***디스크는 움직이는 부품들이 있고, 매우 정밀하므로 고장 나기 쉽다.***

- 때로는 고장이 심해서 디스크 전체를 교체해야 할 때도 있나, 보통은 한 두개 섹터에 결함이 생긴다.
    - 어떤 경우는 공장에서 출고될 때 이미 **손상 블록**을 가지고 나오는 수도 있다.
        - **손상된 블록들은 디스크와 컨트롤러에 따라 다양한 방법으로 처리된다.**

</br>

> **구형 IDE 컨트롤러를 가진 디스크는 손상된 블록들을 수동으로 처리한다.**
> 
- 한 가지 전략은 디스크 포맷 중에 디스크를 스캔하여 손상된 블록이 있는지 검사하는 것이다.
    - 발견된 손상된 블록은 사용 불가라고 표시하여 파일 시스템이 그 블록을 할당하지 않도록 한다.

</br>

> **더욱 정교한 디스크는 손상된 디스크 블록을 더 현명하게 처리한다.**
> 
- 컨트롤러는 손상 블록의 리스트를 유지한다.
    - 리스트는 공장에서 저수준 포맷하는 동안 초기화되고, 디스크가 사용되는 동안 계속 유지된다.
    - 저수준 포매팅은 운영체제가 볼 수 없는 예비 섹터를 남겨 놓는다.
    - 컨트롤러는 예비 섹터 중 하나를 손상된 섹터와 교체 시킨다.

</br>

→ 이러한 기법을 **섹터 예비, 또는 섹터 포워딩이라고 한다!**

</br>

> **일반적으로 손상된 섹터는 아래와 같이 처리된다.**
> 
1. 운영체제가 논리 블록 87을 읽으려고 시도한다.
2. 컨트롤러가 ECC를 계산한 결과, 그 섹터가 손상된 것을 알게 된다.
    - 이 사실을 입출력 오류로 운영체제에 보고한다.
3. 장치 컨트롤러가 손상된 섹터를 예비 섹터로 교체한다.
4. 그 후에는 논리적 블록 87이 요청될 때마다 변경된 새로운 섹터 주소로 가게 된다.

</br>

> **예비 섹터를 관리하는 다른 방안으로서, 일부 컨트롤러는 섹터 밀어내기에 의해 손상 섹터를 처리할 수 있도록 한다.**
> 
- 복구 가능한 **연성 에러**는 블록 데이터의 사본이 만들어지고 블록이 스페어 또는 슬립 되는 장치 활동을 트리거할 수 있다.
    - 그러나 복구가 안되는 **경성 에러**는 데이터를 잃게 된다.

---

## 6. 스왑 공간 관리

→ ***스왑 공간 관리는 운영체제가 수행해야 할 또 다른 하위 수준의 작업이다.***

- 가상 메모리는 보조저장장치를 메인 메모리의 확장된 공간으로 사용한다.
    - 그러나, 드라이브는 메모리보다 훨씬 더 느리므로 스왑 공간은 가상 메모리 성능에 커다란 영향을 미친다.

</br>

### 스왑 공간 사용

→ ***사용하는 메모리 관리 알고리즘에 따라 스왑 공간은 운영체제마다 다양한 방법들로 운영된다.***

</br>

> **스왑 공간은 예상보다 크게 잡는 것이 안전하다!**
> 
- 시스템을 운영하다가 스왑 공간이 바닥나면 프로세스들을 도중에 중단시켜야 하고, 최악의 상황에는 시스템 전체가 crash 될 수 있다.
    - 몇명 운영체제는 권장하는 예약 스왑 공간 크기가 있다.

</br>

### 스왑 공간 위치

→ ***스왑 공간은 일반 파일 시스템이 차지하고 있는 공간 안에서 만들어 줄 수도 있고, 별도의 파티션을 만들어 사용할 수도 있다.***

- 대신에, 스왑 공간은 별도의 **raw 파티션**에 만들어질 수 있다.
    - 그리고 일반 파일이나 디렉터리는 이 공간에 저장되지 않는다.
    - 이 파티션은 별도의 스왑 관리루틴에 의해 스와핑을 하는 데에만 사용된다.
    - 스왑 관리 투린은 공간 효율성 보다는 속도 효율성을 최적화하기 위한 알고리즘을 사용한다.
        - 스왑 공간은 사용될 때 파일 시스템보다 훨씬 자주 접근되기 때문이다.

</br>

> **일부 운영체제들은 유연하게 별개의 파티션과 파일 시스템 공간 모두에 스왑이 가능하다.**
> 
- ex) Linux
    - 그리고, 그 중 어느 방법으로 스왑 공간을 할당할지를 관리자가 결정하도록 한다.

</br>

→ **파일 시스템을 사용하면 할당과 관리가 편리하고, 파티션을 사용하면 성능이 좋아진다!**

---

## 7. 저장장치 연결

→ ***컴퓨터는 호스트에 연결하는 방식, 네트워크로 연결된 저장장치, 클라우드 저장장치 등 3가지 방법으로 보조저장장치에 접근한다.***

</br>

### 호스트 연결 저장장치

→ ***호스트 연결 저장장치는 로컬 I/O 포트를 통해 액세스 되는 저장장치이다.***

- 이 포트들은 **SATA**와 같은 몇 가지 기술을 사용한다.
    - 통상적인 시스템은 하나, 또는 몇 개의 SATA 포트를 가진다.

</br>

> **고성능 워크스테이션과 서버는 일반적으로 더 많은 저장장치가 필요하거나 공유해야 하므로, 광섬유 채널(FC)과 같은 더 정교한 I/O 아키텍처를 사용한다.**
> 
- FC는 광섬유, 또는 4-선 구리 케이블 위에서 작동할 수 있는 고속 직렬 아키텍처이다.
    - 넓은 주소 공간과 스위치 기능이 있는 특성 때문에 다수의 호스트와 저장장치가 기본 망에 연결되어 I/O 통신에 큰 유통성을 제공한다.

</br>

> **다양한 저장장치가 호스트 연결 저장장치로 사용하기에 적합하다.**
> 
- 그중에는 HDD, NVM, CD, DVD, Blu-ray 및 테이프 드라이브, **SAN** 등이 있다.

</br>

### 네트워크 연결 저장장치

→ ***NAS는 네트워크를 통해 저장장치에 대한 액세스를 제공한다.***

- **NAS 장치**는 특수 목적 저장장치 시스템이거나 네트워크의 다른 호스트에 저장장치를 제공하는 일반 컴퓨터 시스템일 수 있다.
    - 클라이언트는 **원격 프로시저 호출 인터페이스**를 통해 네트워크 연결 저장장치에 액세스 한다.
    - RPC는 IP 네트워크 상에서 TCP, 또는 UDP를 통해 전달된다.

<img width="424" alt="image" src="https://user-images.githubusercontent.com/106216912/222918897-744d9048-4dec-4f62-9b96-5e56545e6ff1.png">

</br>

> **CIFS 및 NFS는 이러한 프로토콜을 사용하여 NAS에 액세스 하는 호스트 간에 파일을 공유할 수 있도록 다양한 락킹 기능을 제공한다.**
> 
- ex) 여러 NAX 클라이언트에 로그인한 사용자는 모든 해당 클라이언트에서 동시에 홈 디렉터리에 액세스 할 수 있다.

</br>

> **네트워크에 부착된 저장장치는 LAN상의 모든 컴퓨터에 저장장치 풀(pool)을 로컬 호스트 저장장치를 사용하는 것과 같은 쉬운 네이밍과 접근 기법으로 공유할 수 있는 편리한 방법을 제공한다.**
> 
- 그러나 효율적이지 못하고, 일부 직접 부착된 저장장치보다 낮은 성능을 가지고 있다.

</br>

> **최근의 iSCSI는 네트워크로 연결된 저장장치 프로토콜이다.**
> 
- 핵심은, SCSI 프로토콜을 전송하기 위해 IP 프로토콜을 사용한다.
    - 따라서, SCSI 케이블들이 아닌 네트워크가 호스트들과 호스트들의 저장장치를 연결한다.
    - 결과적으로, 호스트들은 호스트들에 직접 연결되지 않은 저장장치들도 직접 연결된 것처럼 저장장치를 사용한다.

</br>

### 클라우드 저장장치

→ ***클라우드 제공 업체가 제공하는 것 중 하나는 클라우드 저장장치이다.***

- 네트워크 연결 저장장치와 유사하게 클라우드 저장장치는 네트워크를 통해 저장장치에 액세스할 수 있다.
    - 이 저장장치는 유료로 저장장치를 제공하는 원격 데이터 센터에 인터넷, 또는 다른 WAN을 통해 접속하여 액세스 된다.

</br>

> **NAS와 클라우드 저장장치의 또 다른 점**
> 
→ ***저장장치가 접근되는 방식과 사용자에게 제공되는 방식***

- **NAS**
    - CIFS 또는 NFS 프로토콜을 사용하면 그저 다른 파일 시스템인 것처럼 접근되고, iSCSI 프로토콜을 사용하면 raw 블록 장치로 접근된다.
    - 대부분의 운영체제에는 이러한 프로토콜이 통합되어 있으며, 다른 저장장치와 동일한 방식으로 NAS 스토리지를 제공한다.

</br>

- **클라우드 저장장치**
    - API 기반이며, 프로그램은 API를 사용해서 저장장치에 접근한다.

</br>

> **기존 프로토콜 대신 API를 사용하는 한 가지 이유**
> 
- WAN의 지연시간 및 장애 시나리오이다.
    - NAS 프로토콜은 LAN에서 사용하도록 설계되었으며, WAN보다 지연시간이 짧고 저장장치 사용자와 장치 간의 연결이 끊어질 가능성이 훨씬 작다.
    - 클라우드 저장장치를 사용하면 장애가 발생할 가능성이 높으므로, 연결이 복원될 때까지 응용은 액세스를 일시 중지한다.

</br>

### SAN과 저장장치 배열

→ ***네트워크에 부착된 저장 시스템의 단점 중 하나는, 저장장치의 입출력 연산 시 데이터 네트워크의 대역폭을 소비하는 점이고, 이로 인해 네트워크 통신의 지연을 증가시킨다.***

- 이 문제는 특별히 규모가 큰 클라이언트 서버 구축에 심각한 영향을 준다.

</br>

> **SAN은 서버들과 저장장치 유닛들을 연결하는 사유 네트워크이다.**
> 
<img width="484" alt="image" src="https://user-images.githubusercontent.com/106216912/222918944-413c45f4-b1f3-490a-8299-746c486cc8fe.png">

- **SAN의 장점 : 유연성**
    - 여러 호스트와 저장장치가 같은 SAN에 부착될 수 있고, 저장장치는 동적으로 호스트에 할당될 수 있다.
    - 저장장치 배열은 RAID 형식으로 보호되거나, 보호되지 않는 드라이브일 수 있다.
    - SAN 스위치는 호스트와 저장장치 간의 액세스를 허용하거나 금지한다.

</br>

> **저장장치 배열은 SAN 포트, 네트워크 포트 또는 둘 다를 포함하는 특수 목적 장치이다.**
> 
<img width="430" alt="image" src="https://user-images.githubusercontent.com/106216912/222918955-e18e397a-6876-4749-9c53-f63e2b678887.png">

- 또한, 데이터를 저장하는 드라이브와 저장장치를 관리하고 네트워크를 통해 저장장치에 액세스할 수 있는 컨트롤러를 포함한다.
    - 컨트롤러는 CPU, 메모리 및 배열의 기능을 구현하는 소프트웨어로 구성된다.
    - 배열의 기능에는 네트워크 프로토콜, 사용자 인터페이스, RAID 보호, 스냅숏, 복제, 압축, 중복 제거 및 암호화 등이 있다.

</br>

> **FC는 가장 일반적인 SAN 내부연결 방법이다.**
> 
- 또 다른 내부 연결 구조는 **InfiniBand(IB)** 라고 하는 특수 목적의 버스 아키텍처이다.
    - 이것은 서버들과 스토리지 유닛들을 연결하는 고속의 내부연결망을 지원하는 하드웨어와 소프트웨어을 장착한다.

---

## 8. RAID 구조

→ ***RAID(redundant array of inexpensive disk)라고 불리는 다양한 디스크 구성 기술은 일반적으로 성능과 신뢰성 이슈를 해결하는 데 역점을 두고 있다.***

- 현재는 경제적인 이유보다는 높은 신뢰성과 높은 데이터 전송률 때문에 사용된다.

</br>

### 중복으로 신뢰성 향상

→ ***HDD RAID의 신뢰성을 생각해 본다.***

- N개의 디스크로 구성된 세트에 다수의 디스크에 오류가 발생할 확률은 하나의 디스크의 오류 확률보다 훨씬 크다.
    - 데이터를 오직 한 복사본만 둔다면, 각 디스크 오류를 엄청난 양의 데이터 손실을 발생시키고 그와 같은 높은 비율의 데이터 손실은 허용될 수 없다.

</br>

> **따라서 신뢰성의 문제 해결 방안은 중복을 허용하는 것이다!**
> 
- 이는 분실된 정보를 리빌드하기 위해 오류의 경우마다 사용되는 별도의 정보를 저장한다.
    - 따라서, 디스크가 고장 나더라도 데이터가 손실되지 않는다.

</br>

> **중복을 도입하는 가장 간단하지만 비용이 많이 드는 접근 방법은, 모든 드라이브의 복사본을 만드는 것이다.**
> 
- 이 기술을 **미러링(mirroring)** 이라고 불린다.
    - **미러드 볼륨** : 미러링을 사용하는 경우, 하나의 논리 디스크는 두 개의 물리 드라이브로 구성되고, 모든 쓰기 작업은 두 드라이브에 모두에서 실행된다.

</br>

> **미러드 볼륨의 MTBF는 두 가지 요소에 좌우된다.**
> 
1. **단일 드라이브의 MTBF**
2. 손상된 드라이브를 교체하고 다시 저장하는 데 소요되는 **평균 수리 시간**

</br>

→ **미러드 드라이브 시스템은 싱글 시스템보다 더 높은 신뢰성을 제공해 준다!**

- 하지만 미러드 드라이브조차도 두 드라이브의 같은 블록에 쓰기가 동시에 진행되고, 완전히 쓰이기 전에 전원 고장이 발생한다면, 두 블록은 불완전한 상태가 된다.
    - 해결책 : 첫 번째 드라이브에 먼저 작업을 마치고, 다음 드라이브에 쓰는것
    - 해결책 : 비휘발성 캐시를 RAID 배열에 두는 것

</br>

### 병렬성을 이용한 성능 향상

→ ***여러 드라이브를 사용할 경우 여러 드라이브에 걸쳐 데이터 스트라이핑을 사용해서, 전송 비율을 향상할 수 있다.***

- 데이터 스트라이핑은 여러 드라이브에 각 바이트의 비트를 나누어 저장함으로써 구성된다.
    - 이러한 스트라이핑을 비트 레벨 스트라이핑이라고 한다!

</br>

> **비트 레벨 스트라이핑은 8의 배수, 또는 8의 약수로 드라이브 개수를 일반화할 수 있다.**
> 
- ex) 4개의 드라이브로 구성된다면, `i` 번째 비트와 각 바이트의 `4+i` 비트는 `i` 번째 드라이브로 간다.
    - **블록 단위 스트라이핑**에서는 파일의 블록은 여러 드라이브에 거쳐 스트라이핑된다.

</br>

→ 요약하자면, 디스크 시스템에서 병렬성의 두 가지 목적이 있다.

1. 부하 균등화를 이용하여 여러 작은 액세스의 처리량을 높인다.
2. 규모가 큰 액세스의 응답 시간을 줄인다.

</br>

### RAID 레벨

→ ***패리티 비트와 디스크 스트라이핑을 결합하여 적은 비용으로 중복을 허용하는 많은 기법이 제안되었다.***

- 이러한 기법은 각기 다른 가격 대 성능 비를 가지고 있으며, **RAID 레벨**로 분류될 수 있다.
    - 그림에서 표현된 모든 경우는 4개의 드라이브 분량의 데이터를 저장하며, 이외의 드라이브는 오류 복구 시 필요한 중복 정보를 저장하는 드라이브로 사용된다.

</br>

<img width="332" alt="image" src="https://user-images.githubusercontent.com/106216912/222951691-5108fa63-6041-4590-99b6-abcb166780d0.png">

> **RAID 레벨 0 : 블록 레벨로 스트라이핑하는 드라이브 구성**
> 
- 미러링이나 패리티 비트 같은 어떤 중복 정보도 가지고 있지 않다.

</br>

> **RAID 레벨 1 : 드라이브 미러링을 사용한다.**
> 

</br>

> **RAID 레벨 4 : 메모리-스타일 오류 수정 코드 구성이라고도 한다.**
> 
- ECC는 드라이브에 걸쳐서 블록을 스트라이핑하여 저장장치 배열에서 직접 사용될 수 있다.

</br>

- ECC 블록이 하나만 있어도 RAID 4는 실제로 오류를 수정할 수 있다.
    - 드라이브 컨트롤러는 섹터가 올바르게 읽혔는지 여부를 감지할 수 있으므로, 단일 패리티 블록을 사용해서 오류를 수정하고 감지할 수 있다.
    - 운영체제가 블록보다 작은 데이터를 쓰려면, 블록을 읽고 새 데이터로 수정한 후 다시 써야 한다. → **읽기-수정-쓰기 주기**라고 한다.

</br>

- RAID 레벨 4는 동일한 데이터 보호 기능을 제공하면서, 레벨 1과 비교해 두 가지 장점이 있다.
    1. 여러 일반 드라이브에는 하나의 패리티 드라이브만 필요하기 때문에 저장장치 오버헤드가 줄어든다.
    2. 일련의 블록에 대한 읽기 및 쓰기가 N-way 데이터 스트라이핑으로 인해 여러 드라이브에 분산되므로, 블록 세트를 읽거나 쓰는 전송 속도는 N배 빠르다.

</br>

- RAID 4 및 모든 패리티 기반 RAID 레벨의 성능 문제는, XOR 패리티 계산 및 기록 비용이다.
    - 이 오버헤드로 인해 패리티를 사용하지 않는 RAID 배열보다 쓰기 속도가 느려질 수 있다.
    - 최신 범용 CPU는 드라이브 I/O와 비교해 매우 빠르므로, 성능 저하를 최소화할 수 있다.

</br>

> **RAID 레벨 5 : 데이터와 패리티를 모든 `N+1` 드라이브에 분산시킨다.**
> 
- N개 블록의 집합에 대해 각 블록을 위해 하나의 드라이브가 패리티를 저장하고, 다른 드라이브들이 데이터를 저장한다.
    - 패리티를 모든 드라이브에 분산시킴으로써, 하나의 패리티 드라이브에 대한 과도한 집중을 막을 수 있다.

</br>

> **RAID 레벨 6 : P + Q 중복 기법이라고 불리기도 하는데, 여러 디스크 오류에 대비하기 위해 추가의 중복 정보를 저장한다.**
> 
- 패리티 비트를 사용하는 대신에 **Galois field 수학**과 같은 에러 교정 코드가 Q를 계산하는 데 사용된다.
    - 시스템은 2개의 드라이브 오류를 허용할 수 있다.

</br>

> **다차원 RAID 레벨 6 : 일부 정교한 저장장치 배열은 RAID 레벨 6을 증폭시킨다.**
> 

</br>

> **RAID 레벨 0 + 1과 1 + 0 : RAID 0과 RAID 1을 조합한 것으로, 신뢰성과 높은 성능을 제공한다.**
> 
- 하지만, 구축 비용이 많이 소요된다.

</br>

- 또다른 RAID 변형은 RAID 레벨 1 + 0으로, 드라이브는 쌍으로 미러링 된 후, 결과 미러링 된 쌍이 스트라이프 된다.

<img width="394" alt="image" src="https://user-images.githubusercontent.com/106216912/222951752-382417a1-08de-4b12-aa69-061ab80e7d04.png">

</br>

> **스냅숏(snapshot)이나 복제(replication)와 같은 다른 특징들이 상기의 계층에 구현될 수 있다.**
> 
- **스냅숏** : 마지막 갱신이 일어나기 전의 파일 시스템의 모습
- **복제** : 중복성과 복구를 위해 자동으로 분리된 지역에 같은 내용을 쓰는 것

</br>

→ **스냅숏이나 복제와 같은 구현은 RAID가 구현된 계층에 의해 달라진다.**

</br>

### RAID 레벨 선택

→ ***시스템 설계자는 어떤 RAID 레벨을 선택해야 하는가?***

- 고려사항 중 하나는 **복구 능력**이다.
    - 드라이브가 고장 나면, 데이터 복구 시간이 오래 걸릴 수 있다.
    - 복구 성능은 고장 사이의 평균 시간에 영향을 준다.

</br>

> **복구 능력은 RAID 레벨에 따라 천차만별이다.**
> 
- 복구는 다른 드라이브에 모든 정보를 복사하는 RAID 1이 가장 쉽다.
    - 다른 레벨에서는 오류 드라이브의 정보를 복구하기 위해, 다른 모든 드라이브에 접근해야 한다.

</br>

> **RAID 레벨 0은 데이터 손실이 중요하지 않은 고성능 응용 프로그램에 사용된다.**
> 
- 또한, RAID 시스템 설계자와 관리자들은 여러 다른 결정을 내려야 한다.

</br>

### 확장

→ ***RAID의 개념은 심지어 무선 시스템에 데이터를 브로드캐스트하는 일을 포함하여, 여러 저장장치에 적용된다.***

- 테이프의 경우, 여러 테이프 중 하나가 손상을 입어도, RAID 구조는 데이터를 복사할 수 있다.
    - 데이터 브로드캐스팅의 경우 데이터의 블록은 작은 유닛으로 나누어지고, 패리티 유닛과 함께 브로드캐스팅된다.

</br>

### RAID의 문제점들

→ ***RAID는 운영체제나 사용자를 위해 데이터의 사용을 항상 보장하지 않는다.***

- RAID는 물리적 매체의 오류는 보호하지만, 다른 하드웨어나 소프트웨어 오류는 보호하지 못한다.
    - 하드웨어 RAID 컨트롤러 오류 또는 소프트웨어 RAID 코드 버그로 인해 전체 데이터가 손실될 수 있다.

</br>

> **Solaris ZFS 파일 시스템은 체크섬을 사용하여 이러한 문제를 해결하기 위해 혁신적인 접근 방식을 취한다.**
> 
- ZFS는 모든 블록의 내부적인 체크섬을 유지한다.
    - 이 체크들을 체크섬 된 블록과 함께 유지하는 것이 아니라, 블록을 가리키는 포인터와 함께 저장한다.

<img width="356" alt="image" src="https://user-images.githubusercontent.com/106216912/222951799-08fa0c1e-abcb-467e-b3da-e1f7e8b167b2.png">

</br>

> **RAID 구현과 연관된 또 다른 이슈는 융통성의 부족이다.**
> 
- 저장장치 배열이 20개의 디스크 집합이 하나의 커다란 RAID 집합으로 동작하는 것을 허용한다고 해도, 또 다른 이슈가 생길 수 있다.
    - 다양한 크기의 여러 볼륨을 디스크 집합 위에 생성할 수 있을 것이다.
    - 하지만, 몇몇 볼륨 관리자는 볼륨의 크기를 변경하는 것을 허용하지 않는다.
        - 그러한 경우, 부적당하게 짝지어진 파일 시스템 문제에 봉착하게 된다.

</br>

> **ZFS는 파일 시스템 관리와 볼륨 관리를 하나의 단위로 묶음으로써, 전통적인 분리 방식이 허용하는 것에 비해 더 많은 기능을 제공한다.**
> 
- 드라이브 또는 드라이브의 파티션은 RAID 집합을 통해 집합적으로 저장장치의 풀을 구성한다.
    - 이 풀은 하나, 또는 그 이상의 ZFS 파일 시스템을 저장할 수 있다.
    - 풀의 가용 공간 전체가 풀 안의 모든 파일 시스템을 위해 사용될 수 있다.

<img width="256" alt="image" src="https://user-images.githubusercontent.com/106216912/222951824-2de1279f-0457-4269-98d0-ba8590bf02e8.png">

</br>

### 객체 저장소

→ ***데이터 저장장치에 대한 또 다른 접근법은, 저장장치 풀로 시작하여 해당 풀에 객체를 배치하는 것이다.***

- 이 방법은 풀로 탐색하고, 해당 객체를 찾을 방법이 없다는 점에서 파일 시스템과 다르다.
    - 따라서, 객체 지향은 컴퓨터 지향적이며, 프로그램에서 사용하도록 설계되었다.

</br>

> **일반적인 순서는 다음과 같다.**
> 
1. 저장장치 풀 내에 객체를 생성하고 객체 ID를 받는다.
2. 필요할 때 객체 ID를 통해 객체에 접근한다.
3. 객체 ID를 통해 객체를 삭제한다.

</br>

> **HDFS(Hadoop file system) 및 Ceph와 같은 객체 저장소 관리 소프트웨어는 객체를 저장할 위치를 결정하고 객체 보호를 관리한다.**
> 
- 일반적으로, 이는 RAID 배열이 아닌 상용 하드웨어에서 발생한다.
    - 객체 저장소는 일반적으로 고속 랜덤 액세스가 아닌 대량 저장장치에 사용된다.
- 객체 저장소는 **수평 확장성**의 이점이 있다.
    - 즉, 저장장치 배열에는 고정된 최대 용량이 있지만, 객체 저장소에 용량을 추가하기 위해 내부 디스크 또는 연결된 외부 디스크가 있는 컴퓨터를 더 추가하고 저장장치를 풀에 추가하기만 하면 된다.

</br>

> **객체 저장소의 또 다른 주요 특징은 각 객체가 내용에 대한 설명을 포함하고 있기 때문에, 자체 설명하고 있다는 것이다.**
> 
- 실제로 객체 저장소는 콘텐츠를 기반으로 검색할 수 있기 때문에, **콘텐츠 주소지정가능 저장소**라고도 한다.
    - 콘텐츠의 형식은 설정되어 있지 않으므로, 시스템이 저장하는 것은 **구조화되지 않은 데이터**이다.